\documentclass[11pt]{article}
\usepackage{xspace}
\usepackage{cprotect}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{tikz-qtree}

\newcommand{\alternativeMergeOrder}{\texttt{alternativeMergeOrder}\xspace}


\title{A Statistical Comparison of Some Theories of NP~Word~Order}
\author{Richard Futrell, Roger Levy, and Matthew Dryer}
\date{\today}

\begin{document}
\maketitle
 
\begin{abstract}
  A frequent object of study in linguistic typology is the order of elements \{determiner, adjective, numeral, noun\} in the noun phrase. The goal is to predict the relative frequencies of these orders across languages. Here we use Poisson regression to statistically compare some prominent accounts of this variation. We compare feature systems derived from \citet{cinque2005deriving} to feature systems given in \citet{cysouw2010towards} and Dryer (in prep). In this setting, we do not find clear reasons to prefer the model of \citet{cinque2005deriving} or Dryer (in prep), but we find both of these models have substantially better fit to the typological data than the model from \citet{cysouw2010towards}.
\end{abstract}

\section{Introduction}

A frequent object of study in linguistic typology is the variation in the order of elements inside the noun phrase (NP) across languages.
In particular, much work has focused on predicting the relative frequencies across languages of orders of the elements \{determiner, adjective, numeral, noun\}.
Table~\ref{tab:order-freqs} shows the relative frequencies of different orders for these elements across languages (assuming each language exhibits only one dominant order) according to data given in Dryer (in prep).
In this table, $D$ stands for determiner, $N$ stands for numeral, $A$ stands for adjective, and $n$ stands for noun.
Genera counts are the counts of linguistic genera showing a certain order; adjusted frequencies are calculated using a methodology described in Dryer (in prep).

\begin{table}
  \centering
  \begin{tabular}{|l|r|r|}
    \hline
    Order & Adjusted frequency & Genera count \\
    \hline
    $DNAn$  & 21.67 & 57 \\
    $nAND$  & 18.49 & 84 \\
    $DnAN$  & 16.65 & 38 \\
    $DNnA$  & 13.52 & 31 \\
    $NnAD$  &  9.00 & 28 \\
    $nADN$  &  7.94 & 19 \\
    $nDAN$  &  7.00 & 11 \\
    $DnNA$  &  7.00 & 10 \\
    $nNAD$  &  6.65 &  9 \\
    $NAnD$  &  4.00 &  5 \\
    $nDNA$  &  3.73 &  5 \\
    $DAnN$  &  3.40 &  8 \\
    $AnND$  &  3.00 &  3 \\
    $NnDA$  &  3.00 &  3 \\
    $NDAn$  &  3.00 &  3 \\
    $AnDN$  &  2.00 &  3 \\
    $DANn$  &  2.00 &  2 \\
    $nNDA$  &  1.00 &  1 \\
    $NADn$  &  0.00 &  0 \\
    $NDnA$  &  0.00 &  0 \\
    $ADnN$  &  0.00 &  0 \\
    $ADNn$  &  0.00 &  0 \\
    $ANDn$  &  0.00 &  0 \\
    $ANnD$  &  0.00 &  0 \\
    \hline
  \end{tabular}
  \caption{NP orders and their adjusted frequencies across languages and their counts across genera, as given in Dryer (in prep).}
  \label{tab:order-freqs}
\end{table}

Here we consider three proposals from the literature on how to explain these frequencies.
We compare these proposals statistically in a log-linear framework based on how well they can predict the typological data given in Table~\ref{tab:order-freqs}.

We consider three proposals from the literature: those given in Dryer (in prep), \citet{cysouw2010towards} and \citet{cinque2005deriving}.
The first two of these theories are featural in nature: they associate each order with a set of marked features, and claim that orders with more marked features will be less frequent.
The last model, that of \citet{cinque2005deriving}, is derivational in nature: it gives a generative model for how certain word orders arise, where certain decisions in the generative process are considered marked. Orders that require more marked operations to be generated are claimed to be less frequent.
We reduce the last model to a featural model, and then compare which model provides a feature system which can best predict the typological data when the features have different degrees of markedness.

\section{Method}

\subsection{Basics}
We consider each proposal from the literature to define a feature system, and compare the ability of each feature system to predict the observed frequencies of orders.
To do so, we use Poisson regression, as first used in \citet{cysouw2010towards}.
In Poisson regression we represent each langauge with a set of $m$ binary-valued features, and say that the expected frequency $F$ of a language in a sample of $k$ languages is given by:
\begin{align}
  \nonumber
  &F = e^V \\
  \nonumber
  &V = w_b + w_1 \cdot f_1 + w_2 \cdot f_2 + ... + w_m \cdot f_m,
\end{align}
where $f_i$ is an indicator variable with value $0$ when the $i$th feature is $-$, and $1$ when the $i$th feature is $+$, and where the weights $w_b$ and $w_1, ..., w_m$ are those that maximize the probability of the observed counts of languages.

Performing this procedure tells us how well it is possible to predict languages in this framework given a certain set of features.
Feature weights may be negative, in which case they can be considered \emph{marked}: the presence of these features is disfavored under the model.
Since features get different weights, the model implements different degrees of markedness per feature, as in Harmonic Grammar \citep{smolensky2006harmonic}.
The model finds the degrees of markedness which best predict the data given the feature system.

\subsection{Feature systems under comparison}

Within this framework, we compare three feature systems: (1) the system in Dryer (in prep), (2) the system in \citet{cysouw2010towards}, and (3) the theory of \citet{cinque2005deriving}. Cinque's theory is not phrased in terms of features, so we use two reductions of his theory to features: those presented in \citet{merlo2015predicting} and our own, shown in Figure~\ref{fig:cinque-model}.

\subsection{Dependent variables}

We apply Poisson regression to predict two quantities. First, we try to predict the \emph{adjusted frequency} of each order, as given in Dryer (in prep) and shown in Table~\ref{tab:order-freqs}. Second, we try to predict the counts of genera given in the same paper.
We round the adjusted frequencies and genera counts to the nearest integer in order to satisfy the assumptions of Poisson regression (natural number-valued dependent variable).

\subsection{Basis for model comparison}

We compare models using log likelihood, the log probability assigned to the observed frequencies under the model.
A model fits the data well when it assigns high probability to the data, so high log likelihood indicates a good fit.
When log likelihood for different models is close, we can also compare them by their degrees of freedom, which is the number of free parameters in the model.
In general, simpler models with fewer parameters are preferable over ones with more parameters.

\subsection{Notes on Featurization of \citet{cinque2005deriving}}
\label{sec:notes-feat-citetc}

Special care is needed when reducing the theory of \citet{cinque2005deriving} to features so that it can be compared with the other theories in a regression framework.

The theory of \citet{cinque2005deriving} is not featural in nature,
but rather derivational. In this model, orders are built up by a
generative process that makes decisions in a certain order; whereas in
featural models, orders are assigned scores based on features that
have no intrinsic order.  For example, the centerpiece of the theory
of \citet{cinque2005deriving} is the claim that the Merge order of
D$>$N$>$A$>$n is universal, and that the Linear Correspondence Axiom
(LCA) of Kayne (1994) \textbf{[TODO: put ref in bibfile]} holds, such
that every word order must be generated from a base structure of the
form seen in Figure~\ref{fig:cinque-base-order}, plus movement
operations.  Orders that are not derivable from this base structure
are not generated at all in Cinque's theory.  For such orders, the
question of what if any movement operations apply never even arises
under Cinque's theory, in principle.


\begin{figure}[t]
  \centering
  
\Tree [.DemP Dem [.NumP Num [.AdjP Adj NP ] ] ]

  \caption{The universal base structure of D,N,A,n under the theory of
    \citet{cinque2005deriving}.}
  \label{fig:cinque-base-order}
\end{figure}


For example, suppose we think of the Cinque model in terms of features: then \texttt{ViolatesLCA} is a feature that can be $+$ or $-$, and some movement operation is reflected in a feature that can be $+$ or $-$.
For an order that violates the LCA, we would give it value $+$ for \texttt{ViolatesLCA} and $-$ for the movement feature, but this is not a completely correct reflection of Cinque's derivational theory.
The reason is that under the generative process, if a word order
violates the LCA, then the model never even decides whether to perform
a movement operation or not: thus the value of the movement feature
should not be $-$ or $+$, but undefined.  

The theory of \citet{cinque2005deriving} also involves theoretically-derived graded markedness values for operations in the generative process: for instance, total movement is claimed to be unmarked, ``picture of who'' type movement is claimed to be especially marked, and the rest of the movement features are claimed to be marked.
In our methodology, we let the model decide on feature weights (markedness values) without regard to these a priori markedness values.
As such, it is possible that our implementation of the Cinque model
does not reflect its full intent. 

The fact that our weights are derived through fits to the data and not through a priori considerations is especially noteworthy in the case of the feature \texttt{ViolatesLCA} in Cinque's system.
In a literal reading of \citet{cinque2005deriving}, orders which violate the LCA should occur with probability 0, and thus the feature \texttt{ViolatesLCA} should be assigned infinitely negative weight.
In that case the model would assign probability 0 to any data that has nonzero frequency for any such order.
In our work we let the model learn that \texttt{ViolatesLCA} has a large negative weight, without postulating that orders violating the LCA must have probability 0.

Here we represent Cinque's model using features for the sake of
convenience in statistical comparison, while noting that this
introduces the issues above.  And there is a further issue that should
be noted.  If Merge orders other than that seen in
Figure~\ref{fig:cinque-base-order} are allowed, then not only are
otherwise impossible word orders generable by postulating that those
word orders are generated as a base structure; additionally, word
orders that were already generable under Cinque's theory also wind up
with new derivations from different base structures.  Taking this
multiplicity of possible derivations into account increases the
complexity of the problem of inferring feature weights, and takes it
outside the scope of standard Poisson regression or other generalized
linear models.  This is a difficulty shared by the modeling approaches
of \citet{cysouw2010towards} and \citet{merlo2015predicting}.  For
expediency, however, we follow previous work in treating every word
order not derivable from the D$>$N$>$A$>$n base structure of
Figure~\ref{fig:cinque-base-order} as being derived by an alternative
Merge order (so that \alternativeMergeOrder\texttt{=T}) with no
movement, ignoring alternative derivations.


We also note that in formalizing Cinque's (2005) theory into a
featural description, we noticed certain unclarities in the text which
affected our formalization.  These are as follows (note that Cinque
uses \textbf{N} where we use \textbf{n}, and \textbf{Num} where we use
\textbf{N}:
%
\begin{itemize}
\item  Cinque describes order AnDN (his (k)) as involving two marked
  options: ``derivation with raising of NP plus \textbf{pied-piping of
    the picture of who type} of the lowest modifier (A), followed by
  \textbf{raising} of [A N] \textbf{without pied-piping} around both
  Num and Dem'' (emphasis ours).  Technically speaking, it is not
  clear whether this latter raising (without pied-piping) should count as marked
  in his system, since the only relevant parameter of movement 
   is ``Movement of NP without pied piping'' (his (7biii)), but this
   latter raising is movement of AP, not NP.  Nevertheless we followed
   Cinque in assigning this word order a $+$ value for movement
   (of NP) without pied piping.  Additionally, there is an
   inconsistency in \citet{cinque2005deriving} between (7bv), where
   this order is stated to involve partial movement, and (6k), where
   partial movement is not listed as a type of markedness for this
   order. Here we went with (7bv) and listed this order as involving
   \verb-partial_move=+-; we believe that this treatment is the most
   globally consistent overall, on analogy with orders such as NnAD
   which Cinque treats as involving partial movement because there are
   multiple types of movement and the first (raising of NP around A)
   is only partial.
 \item As with AnDN, there is inconsistency between (7bv) and the
   word-order-specific description (6w): in the  former, this order is
   stated to involve partial movement of NP, but in the latter,
   partial movement is not mentioned as a type of markedness.  As with AnDN,
   we listed this order as involving \verb-partial_move=+-.   % Cinque's description for order AnND (his (6w)) is not
   % consistent with that for order AnDN described just above: for AnDN,
   % raising of [A N] around Num and Dem counts as marked (raising
   % without pied-piping), but here, raising of
 \item Cinque describes order nDAN as involving ``extraction of the
   sole NP around Dem'' as the final movement in the derivation, an
   operation appealed to for no other word order and not unambiguously
   categorizable in his parameters of movement in (7b); Cinque does
   not list the number of marked options for this order.
   \citet{cysouw2010towards} dedicates a specific feature to this type
   of movement in his modeling effort; we treat it as a case of
   movement of NP without pied piping.
\end{itemize}
%
Regarding the first two cases, it should be emphasized that
\citet{cinque2005deriving} is far from totally clear about what does and
does not count as partial (and thus marked) movement.  For example,
NnAD is described as involving partial (and thus marked) raising of NP
around A, followed by a second raising that gets the raised
constituent all the way to the left edge, but though nNAD likewise starts
with a partial raising of NP around A (and N) followed by a second
raising that gets the raised constituent all the way to the left edge,
it is not considered to involve partial movement.

Additionally, there are two cases of what we believe are coding errors
by\citet{cysouw2010towards} in his implementation of Cinque's model
(see his Appendix on page 284): Cysouw encodes NnAD and nNAD as
involving NP movement with pied piping of the \emph{whose picture}
variety, but Cinque describes these orders (his (6s) and (6t)) as
involving \emph{picture of who} pied piping instead, which seems
correct to us.

\subsection{Comparison with \citet{merlo2015predicting}}

\citet{merlo2015predicting} conducts a study with similar aims to ours and uses featurizations more or less the same as what we've discussed above.
She uses features to predict frequency classes using a Naive Bayes estimator and an Weighted Averaged One-Dependence (WAODE) estimator, rather than Poisson regression as we use here and as was proposed by \citet{cysouw2010towards}.
As a summary of how this work: \citet{merlo2015predicting} first discretizes the integer-valued word order frequency counts (by langauge or by genus) into 2, 4, or 7 categories; then she learns a model that categorizes language classes by their features according to the classic Naive Bayes formula:
\begin{align*}
  \nonumber
  P(\text{class}|\text{features}) &\propto P(\text{features}|\text{class}) P(\text{class}) \\
  \nonumber
  P(\text{features}|\text{class}) &= \prod_{f_i \in \text{features}} P(f_i | \text{class}),
\end{align*}
where $f_i$ is the value of the $i$th feature in the featurization scheme under consideration; our $f_i$ here are Merlo's $a_i$ in her Equations 1--4, p. 334. A word order is assigned to the frequency class that maximizes the probability $P(\text{class}|\text{features})$ above for that word order's features.
Note that technically these ``features'' are attribute-value pairs, such as $\texttt{Symmetry1=T}$ for the Dryer model or $\texttt{Partial=whose-pp}$ for the Cinque model.
The WAODE model is a bit more complex than the Naive Bayes model but is fundamentally similar.

Our work differs from Merlo's approach on two points:
\begin{itemize}
\item In predicting typological data, we use Poisson regression, which is a discriminative log-linear predictor, rather than Naive Bayes and WAODE, which are generative models.
\item Our models predict integer-valued typological counts, whereas the models in \citet{merlo2015predicting} predict unordered categorically-valued frequency classes.
\end{itemize}

We favor Poisson regression (and more generally log-linear models) over the Naive Bayes/WAODE approach because it allows us to predict more fine-grained typological data and to model the strong intuition that the effects of features on typological frequencies should be monotonic.
In a model where the goal is to classify each language into the categories (e.g.) \{Very Frequent, Frequent, Rare, None\}, there is nothing to prevent a feature from getting weights that favor Very Frequent and None while disfavoring Frequent and Rare.
Examples of this non-monotonicity in feature weights can be seen in Merlo's (2015) Table 11.
This means that model weights from this framework cannot be considered markedness values, which either penalize an order (make it less frequent) or do not.
In addition to making the model weights less interpretable, this non-monotonicity means that the model has the flexibility to take advantage of artifacts of the discretization of word order frequencies into bins.


\subsection{Comparison with \citet{cysouw2010towards}}
\label{sec:comp-with-citetcys}

As stated in Section~\ref{sec:notes-feat-citetc}, our approach here is
very similar to that of \citet{cysouw2010towards}: we use the same
statistical model class and the same theory comparison
(Cinque/Cysouw/Dryer).  The differences are as follows:
%
\begin{itemize}
\item We use the more recent data of Dryer (in prep) rather than the
  earlier data of Dryer (2006);
\item We use the feature set of Dryer (in prep) rather than the
  earlier feature set of Dryer (in prep);
\item We correct what we believe are two featurization errors made by
  Cysouw in featurizing Cinque's theory, and we featurize order nDAN
  differently than Cysouw does (see above).
\end{itemize}


\section{Results}

The results do not give clear grounds for deciding between the Dryer model and the Cinque model, but both of these models come out better than the \citet{cysouw2010towards} model. Whether or not the Dryer model comes out better depends on whether we use the model to predict adjusted frequencies or genera counts.

\subsection{Predicting Adjusted Frequencies}

Table~\ref{tab:af-likelihoods} shows log likelihoods for models predicting adjusted frequency. It also shows the number of parameters (d.f.) in each model. The table shows that Cinque's model slightly outperforms Dryer (in prep) on fitting the data.

\begin{table}
  \centering
  \begin{tabular}{|l|r|r|}
    \hline
    Model & Log likelihood & d.f. \\
    \hline
    Dryer (in prep) & -46.6 & 6 \\
    \citet{cysouw2010towards} & -60.1 & 5 \\
    \citet{cinque2005deriving} (our features) & -44.0 & 7 \\
    \citet{cinque2005deriving} (tied markedness) & -44.6 & 5 \\
    \citet{cinque2005deriving} (Merlo's features) & -46.4 & 8\\
    \hline
  \end{tabular}
  \caption{Log likelihoods of adjusted frequency data under various models, and the degrees of freedom (d.f.) of those models.}
  \label{tab:af-likelihoods}
\end{table}

For a more detailed comparison of model performance, we compared model predictions to observed adjusted frequencies from Dryer (in prep). Figure~\ref{fig:af-predictions} shows model predictions compared against adjusted frequency.

\begin{figure}[ht!]
  \centering
  \includegraphics[scale=.7]{model_plots.pdf}
  \caption{Adjusted frequency of word orders compared to model predictions. In this and all plots, the ``Cinque'' model refers to our featurization of Cinque's theory, including all features in the model without tying weights.}
  \label{fig:af-predictions}
\end{figure}


We wanted to know how much each order contributed to model fit, so in Figure~\ref{fig:af-discrepancies} we show signed $\chi^2$-discrepancies between model predictions and adjusted frequency. The $\chi^2$ discrepancy measures how much the prediction error for each word order contributse to the overall discrepancy between data and model fit; signed $\chi^2$ discrepancy presents this discrepancy in the direction of the discrepancy for each order (whether it under-predicts or over-predicts).
If a model predicts a count of $E_i$ for the $i$th word order and the observed count is $O_i$, then the signed $\chi^2$ discrepancy is:
\begin{align}
  \nonumber
  \frac{(O_i - E_i) \times |O_i - E_i|}
       {E_i}.
\end{align}
The magnitude of the discrepancy corresponds to how much a model is penalized for failing to predict a certain order.

\begin{figure}[ht!]
  \centering
  \includegraphics[scale=.7]{model_discrepancies.pdf}
  \caption{Signed chi-squared discrepancies between model predictions and adjusted frequency. Scores below zero mean that a model underpredicts frequency; scores above zero means that the model overpredicts frequency.}
  \label{fig:af-discrepancies}
\end{figure}

As another way to analyze the results, we show in Figures~\ref{fig:dryer-model} and \ref{fig:cinque-model} the weights assigned to features for different word unders under the Dryer model and the Cinque model.
Here we see that Cinque's model works by strongly penalizing low-frequency orders using the \texttt{ViolatesLCA} feature, and then the differences among the remaining orders are handled by the rest of the features.
The figure also highlights why the Dryer model underpredicts the frequency of $DNAn$ orders: these orders are positive for the \texttt{nadj} feature, which must have a negative weight in order to penalize various low-frequency orders.

\begin{figure}[ht!]
  \centering
  \includegraphics[scale=.7]{dryer_regression_af.pdf}
  \caption{Feature weights from the Dryer (in prep) model when predicting adjusted frequency (first column).}
  \label{fig:dryer-model}
\end{figure}

\begin{figure}[ht!]
  \centering
  \includegraphics[scale=.7]{cinque_regression_af.pdf}
  \cprotect\caption{Feature weights from the \citet{cinque2005deriving} model when predicting adjusted frequency (first column). Note that the feature \verb+whose_pic_move+ comes out to have a positive weight.}
  \label{fig:cinque-model}
\end{figure}

\subsection{Predicting Genera Counts}

Now we turn to models that were trained to predict the genera count data given in Dryer (in prep).
When we use the various feature systems to predict genera counts, we get the following data log-likelihoods, shown in Table~\ref{tab:genera-likelihoods}:

\begin{table}
  \centering
  \begin{tabular}{|l|r|r|}
    \hline
    Model & Log likelihood & d.f. \\
    \hline
    Dryer (in prep) & -61.3 & 6 \\
    \citet{cysouw2010towards} & -106.0 & 5 \\
    Cinque (our features) & -67.9 & 7 \\
    Cinque (tied markedness) & -70.1 & 5 \\
    Cinque (Merlo's features) & -72.9 & 8 \\
    \hline
  \end{tabular}
  \caption{Log likelihoods of genera count data under various models, and the degrees of freedom (d.f.) of those models.}  
  \label{tab:genera-likelihoods}
\end{table}

So when predicting genera, we get the best fit to the data using the set of features from the current work
, followed by Cinque's (2005) features, followed by Cysouw's (2010) features.

We think Cinque's model comes out worse when predicting genera primarily because it underpredicts $NnAD$ orders, whereas the Dryer model gets that order exactly correct. This can be seen in Figure~\ref{fig:genera-predictions}, which shows model predictions, and Figure~\ref{fig:genera-discrepancies}, which shows signed $\chi^2$ discrepancies compared to genera counts.
Figures~\ref{fig:dryer-model-genera} and \ref{fig:cinque-model-genera} show the optimal feature weights for the Dryer and Cinque models, respectively, when predicting genera counts.

\begin{figure}[ht!]
  \centering
  \includegraphics[scale=.7]{model_plots_genera.pdf}
  \caption{Genera counts of word orders compared to model predictions. In this and all plots, the ``Cinque'' model refers to our featurization of Cinque's theory, including all features in the model without tying weights.}
  \label{fig:genera-predictions}
\end{figure}

\begin{figure}[ht!]
  \centering
  \includegraphics[scale=.7]{model_discrepancies_genera.pdf}
  \caption{Signed chi-squared discrepancies between model predictions and genera counts. Scores below zero mean that a model underpredicts frequency; scores above zero means that the model overpredicts frequency.}
  \label{fig:genera-discrepancies}
\end{figure}

\begin{figure}[ht!]
  \centering
  \includegraphics[scale=.7]{dryer_regression_genera.pdf}
  \caption{Feature weights from the Dryer (in prep) model when predicting genera counts (first column).}
  \label{fig:dryer-model-genera}
\end{figure}

\begin{figure}[ht!]
  \centering
  \includegraphics[scale=.7]{cinque_regression_genera.pdf}
  \cprotect\caption{Feature weights from the \citet{cinque2005deriving} model when predicting genera counts (first column). Note that the feature \verb+whose_pic_move+ comes out to have a positive weight.}
  \label{fig:cinque-model-genera}
\end{figure}

\section{Discussion}

The results give clear evidence that the Dryer (in prep) and \citet{cinque2005deriving} model provide feature systems that have better predictive power than the model of \citet{cysouw2010towards}. But in our opinion they do not give strong reason to favor Cinque's model over Dryer's model or vice versa. Although under a certain interpretation Cinque's model can a slightly higher fit to the data, this only holds under one featurization, and it does not hold when predicting genera counts. The analysis suggests that the Dryer model and the Cinque model have roughly similar predictive power, and the current data do not discriminate between them.


\section*{Acknowledgments} 

This work was supported by NSF DDRI grant \#1551543 to R.F.


\bibliography{everything}
\bibliographystyle{apa}
\end{document}

